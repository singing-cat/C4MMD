# C4MMD: Exploring Chain-of-Thought for Multi-modal Metaphor Detection

## Abstract
Metaphors are commonly found in advertising and internet memes. However, the free form of internet memes often leads to a lack of high-quality textual data. Metaphor detection demands a deep interpretation of both textual and visual elements, requiring extensive common-sense knowledge, which poses a challenge to language models. To address these challenges, we propose a compact framework called C4MMD, which utilizes a **C**hain-of-Thought(CoT) method **for** **M**ulti-modal **M**etaphor **D**etection. Specifically, our approach designs a three-step process inspired by CoT that extracts and integrates knowledge from Multi-modal Large Language Models(MLLMs) into smaller ones. We also developed a modality fusion architecture to transform knowledge from large models into metaphor features, supplemented by auxiliary tasks to improve model performance. Experimental results on the MET-MEME dataset demonstrate that our method not only effectively enhances the metaphor detection capabilities of small models but also outperforms existing models. To our knowledge, this is the first systematic study leveraging MLLMs in metaphor detection tasks.
The main process of our method is shown in the following figure.

 ![An illustration of C4MMD using the MLLM for multi-modal metaphor detection.](structure.png "Main structure")



## Table of Contents

- [Installation](#installation)
- [Usage](#usage)
- [Citation](#Citation)

## Installation

You may need to download the following content in advance to use our codeï¼š
1. [MET-Meme Dataset](https://github.com/liaolianfoka/MET-Meme-A-Multi-modal-Meme-Dataset-Rich-in-Metaphors)
2. [InternLM-XComposer](https://github.com/InternLM/InternLM-XComposer) model and it's inference demo(as our demo may out of date, we recommend you to use the latest one.).
3. Pre-trained [Vision](https://huggingface.co/google/vit-base-patch16-224) and [Language](https://huggingface.co/FacebookAI/xlm-roberta-base) model using in our donwstream mordality fusion structure.

After preparing the above stuff, you can download our code through the following lines.
```bash
https://github.com/xyz189411yt/C4MMD.git
cd C4MMD-main
pip install -r requirements.txt
```
**Note:** Due to the use of MLLM in the CoT module, newer versions of Torch and Transformers are required, as shown in requirements.txt. While the C4MMD model does not involve any MLLMs, lower versions of Torch and Transformers can also be used. We are using Torch and Transformers versions 1.13.0 and 4.24.0 when trainging C4MMD.

## Usage

You may need three following three steps to run our model. These three steps correspond exactly to three different Pyhon file.

1. [Data pre-processing](#Data-pre-processing) -> [data_divide.py](data_divide.py)
2. [CoT module for MLLM](#CoT-module-for-MLLM) -> [CoT_module.py](CoT_module.py)
3. [Main training process](#Main-training-process) -> [C4MMD_train.py](C4MMD_train.py)

### Step1: Data pre-processing

Since the original dataset did not divided into training, testing, and validation sets, we provided [data_divide.py](data_divide.py) for dividing the dataset. 
At the same time, we merged the image-text correspondence files and image-label correspondence files in the dataset into one for further use.

You only need to modify the data reading path in this file and run it with the following lines.

```bash
python data_divide.py
```

It should be noted that the data reading path has been displayed in the file. It's fine If you insist on using your data format. We are just showcasing our data processing methods. 
You just need to ensure that the final data format is the same as in the [data](/data) folder to execute the final training process.

### Step2: CoT module for MLLM

[CoT_module.py](CoT_module.py) contains our main contribution.
You need to first modify the read and save paths around line 142, and then run directly using the following command line.

```bash
python CoT_module.py
```
If you successfully execute the [CoT_module.py](CoT_module.py), you will find that a new data file (new_xxx.json) has been generated in the [data](/data) folder. Each example has three additional attributes corresponding to the three modal features generated by MLLM: 
- internlm_img_info: additional image information
- Internlm_text_info: additional text information
- Internlm_mix_info: Additional information after mixing two modalities.

If you find that it cannot run or want to try other MLLMs, you just neet to find the inference demo for the corresponding model and place the CoT module section of our code (after line 137) in the corresponding position.

### Step3: Main training process

If you ultimately process the data in the format of [data](/data), you only need to slightly modify each file path and pre-trained models in [C4MMD_train.py](C4MMD_train.py) to use the following commands to train our model.

```bash
python C4MMD_train.py
```

## Citation
This project is currently contributed by Yanzhi Xu*, Yueying Hua*, Shichen Li and Zhongqing Wang from Natural Language Processing Lab, Soochow University.

```bib
@inproceedings{xu2024C4MMD,
    title = "Exploring Chain-of-Thought for Multi-modal Metaphor Detection",
    author = "Xu, Yanzhi  and
      Hua, Yueying  and
      Li, Shichen  and
      Wang, Zhongqing",
    booktitle = "Proceedings of ACL",
    year = "2024",
}
```
